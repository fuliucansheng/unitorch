[core/cli]
task_name = core/task/megatron/supervised
from_ckpt_dir = ./cache
cache_dir = ./cache
train_file = ./train.tsv
dev_file = ./dev.tsv

# model
[core/model/generation/megatron/gpt]
config_path = https://raw.githubusercontent.com/fuliucansheng/unitorch/refs/heads/master/examples/configs/megatron/gpt.json
vocab_size = 32000
num_experts = 2

# dataset
[core/dataset/megatron/ast]
names = ['encode', 'decode']

[core/dataset/megatron/ast/train]
data_files = ${core/cli:train_file}
preprocess_functions = ['core/process/llama/generation(encode, decode)']

[core/dataset/megatron/ast/dev]
data_files = ${core/cli:dev_file}
preprocess_functions = ['core/process/llama/generation(encode, decode)']

# process
[core/process/llama]
pretrained_name = llama-7b
max_seq_length = 64
max_gen_seq_length = 36

# optim
[core/optim/adamw]
learning_rate = 0.0001

# scheduler
[core/scheduler/linear_warmup]
num_warmup_rate = 0.001

# task
[core/task/megatron/supervised]
tensor_model_parallel_size = 2
pipeline_model_parallel_size = 1
context_parallel_size = 1
model = core/model/generation/megatron/gpt
optim = core/optim/adamw
scheduler = core/scheduler/linear_warmup
dataset = core/dataset/megatron/ast

num_workers = 4
grad_acc_step = 4
seq_length = 100 # 64 + 36

from_ckpt_dir = ${core/cli:from_ckpt_dir}
to_ckpt_dir = ${core/cli:cache_dir}
train_batch_size = 4
dev_batch_size = 8
