[core/cli]
task_name = core/task/supervised
from_ckpt_dir = ./cache
cache_dir = ./cache
train_file = ./train.tsv
dev_file = ./dev.tsv
test_file = ./test.tsv

# model
[core/model/generation/peft/dpo/lora/qwen2_5_vl]
pretrained_name = qwen2_5-vl-3b-instruct
no_repeat_ngram_size = 3
max_gen_seq_length = 512

# dataset
[core/dataset/ast]
names = ['encode', 'image', 'win_decode', 'lose_decode']

[core/dataset/ast/train]
data_files = ${core/cli:train_file}
preprocess_functions = ['core/process/qwen_vl/dpo/generation(encode, core/process/image/read(image), win_decode, lose_decode)']

[core/dataset/ast/dev]
data_files = ${core/cli:dev_file}
preprocess_functions = ['core/process/qwen_vl/generation/inputs(encode, core/process/image/read(image))', 'core/process/qwen_vl/generation/labels(win_decode)']

[core/dataset/ast/test]
names = ['encode', 'image']
data_files = ${core/cli:test_file}
preprocess_functions = ['core/process/qwen_vl/generation/inputs(encode, core/process/image/read(image))']

# process
[core/process/qwen_vl]
pretrained_name = qwen2_5-vl-3b-instruct
max_seq_length = 12800
max_gen_seq_length = 512

[core/process/image]
http_url = http://0.0.0.0:11230/?file={0}

[core/writer/csv]
escapechar = \

# optim
[core/optim/adamw]
learning_rate = 0.0001

# scheduler
[core/scheduler/linear_warmup]
num_warmup_rate = 0.001

# task
[core/task/supervised]
model = core/model/generation/peft/dpo/lora/qwen2_5_vl
optim = core/optim/adamw
scheduler = core/scheduler/linear_warmup
dataset = core/dataset/ast
loss_fn = core/loss/lm
score_fn = core/score/bleu
monitor_fns = ['core/score/bleu', 'core/score/rouge1', 'core/score/rouge2', 'core/score/rougel']
output_header = ['encode']
postprocess_fn = core/postprocess/qwen_vl/detokenize
writer = core/writer/csv

from_ckpt_dir = ${core/cli:from_ckpt_dir}
to_ckpt_dir = ${core/cli:cache_dir}
output_path = ${core/cli:cache_dir}/output.txt
train_batch_size = 4
dev_batch_size = 8
test_batch_size = 8
